{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 20.5/57.6 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------- ----------------- 30.7/57.6 kB 262.6 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 51.2/57.6 kB 327.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 303.6 kB/s eta 0:00:00\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.35.32-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (2.31.0)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 978.3 kB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.7/59.7 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting botocore<1.36.0,>=1.35.32 (from boto3)\n",
      "  Downloading botocore-1.35.32-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Collecting click (from sacremoses)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from sacremoses)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.8-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (68.2.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "     ---------------------------------------- 0.0/167.0 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 112.6/167.0 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------  163.8/167.0 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 167.0/167.0 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 52.5/52.5 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.4/78.4 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading boto3-1.35.32-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.1/139.1 kB 4.2 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.5 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 256.0/273.5 kB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 266.2/273.5 kB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 273.5/273.5 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------- ---------------------------- 256.0/992.0 kB 15.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 307.2/992.0 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 860.2/992.0 kB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/992.0 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 992.0/992.0 kB 5.2 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 897.5/897.5 kB 18.9 MB/s eta 0:00:00\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 37.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.3/12.6 MB 24.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.4/12.6 MB 37.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.0/12.6 MB 37.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.4/12.6 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.4/12.6 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.0/12.6 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.6 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 4.5/11.5 MB 96.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.5/11.5 MB 95.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.5/11.5 MB 95.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.5 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading torch-2.4.1-cp312-cp312-win_amd64.whl (199.4 MB)\n",
      "   ---------------------------------------- 0.0/199.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/199.4 MB 69.7 MB/s eta 0:00:03\n",
      "    --------------------------------------- 4.0/199.4 MB 51.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 6.5/199.4 MB 51.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 8.3/199.4 MB 48.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 10.9/199.4 MB 46.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 12.7/199.4 MB 43.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 15.2/199.4 MB 46.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 17.1/199.4 MB 43.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 19.8/199.4 MB 46.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 21.7/199.4 MB 43.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 24.3/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 26.1/199.4 MB 46.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 28.8/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 30.7/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 33.2/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 35.3/199.4 MB 50.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 37.7/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 39.5/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 42.3/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 44.4/199.4 MB 50.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 47.1/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 49.2/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 51.8/199.4 MB 50.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 54.0/199.4 MB 54.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 56.5/199.4 MB 50.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 58.9/199.4 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 61.4/199.4 MB 50.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 63.9/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 66.2/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 68.9/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 71.4/199.4 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 73.9/199.4 MB 59.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 76.6/199.4 MB 54.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 79.1/199.4 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 81.7/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 84.2/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 86.9/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 89.5/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 91.9/199.4 MB 59.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 94.1/199.4 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 94.1/199.4 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 94.5/199.4 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 97.4/199.4 MB 40.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 99.8/199.4 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 104.7/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 106.0/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 107.3/199.4 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 108.6/199.4 MB 46.9 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 110.0/199.4 MB 43.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 111.3/199.4 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 112.6/199.4 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 114.0/199.4 MB 31.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 115.2/199.4 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.6/199.4 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 117.8/199.4 MB 28.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 119.2/199.4 MB 28.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 120.5/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 122.0/199.4 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 123.2/199.4 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 124.7/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.9/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 127.4/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.6/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 130.1/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 131.4/199.4 MB 28.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 132.8/199.4 MB 29.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.2/199.4 MB 31.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.6/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.9/199.4 MB 29.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.3/199.4 MB 29.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 139.7/199.4 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.1/199.4 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.5/199.4 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 143.8/199.4 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 145.3/199.4 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.7/199.4 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 148.0/199.4 MB 29.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 149.4/199.4 MB 29.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 150.9/199.4 MB 31.1 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.2/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.7/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.1/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.4/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 157.9/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.3/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.8/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.2/199.4 MB 31.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 163.6/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 165.1/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.6/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.9/199.4 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 169.4/199.4 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 170.8/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.1/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 173.7/199.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 175.1/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.6/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.0/199.4 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.5/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.8/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 182.4/199.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.8/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.4/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.6/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.3/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 189.7/199.4 MB 31.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.1/199.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.7/199.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.0/199.4 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.6/199.4 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.9/199.4 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/199.4 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.4/199.4 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.9 MB 52.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 36.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.9 MB 35.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 33.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 33.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 33.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 33.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 28.7 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.7/7.8 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.0/7.8 MB 32.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.8/7.8 MB 33.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/7.8 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.8 MB 33.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 27.7 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.3 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading botocore-1.35.32-py3-none-any.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.9/12.6 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/12.6 MB 33.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.0/12.6 MB 35.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.2/12.6 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.9/12.6 MB 33.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 32.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.1/12.6 MB 34.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "   ---------------------------------------- 0.0/218.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 218.3/218.3 kB 13.9 MB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 62.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.3 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.8-cp312-cp312-win_amd64.whl (379 kB)\n",
      "   ---------------------------------------- 0.0/379.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 379.1/379.1 kB 11.9 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.9/55.9 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.6/2.6 MB 34.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.6 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 27.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.9/25.1 MB 39.7 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.8/25.1 MB 44.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.3/25.1 MB 34.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.3/25.1 MB 34.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/25.1 MB 33.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.0/25.1 MB 30.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.2/25.1 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.1/25.1 MB 28.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.4/25.1 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.3/25.1 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.6/25.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.5/25.1 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.8/25.1 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.0/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.0/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.1/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.2/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.1 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "   ---------------------------------------- 0.0/104.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 104.1/104.1 kB ? eta 0:00:00\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.7/82.7 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.3/286.3 kB 17.3 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 146.7/146.7 kB ? eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.1/1.7 MB 23.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 21.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.1/6.2 MB 33.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.2 MB 26.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.3/6.2 MB 26.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.3/6.2 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.6/6.2 MB 25.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 26.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 22.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.7/111.7 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, mpmath, xxhash, typing-extensions, tqdm, sympy, safetensors, requests, regex, pyparsing, pillow, numpy, networkx, multidict, kiwisolver, joblib, jmespath, fsspec, frozenlist, fonttools, filelock, dill, cycler, click, aiohappyeyeballs, yarl, torch, sacremoses, pyarrow, pandas, multiprocess, huggingface-hub, contourpy, botocore, aiosignal, tokenizers, s3transfer, matplotlib, aiohttp, transformers, boto3, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.8 aiosignal-1.3.1 boto3-1.35.32 botocore-1.35.32 click-8.1.7 contourpy-1.3.0 cycler-0.12.1 datasets-3.0.1 dill-0.3.8 filelock-3.16.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.1 jmespath-1.0.1 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.9.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.3 numpy-2.1.1 pandas-2.2.3 pillow-10.4.0 pyarrow-17.0.0 pyparsing-3.1.4 regex-2024.9.11 requests-2.32.3 s3transfer-0.10.2 sacremoses-0.1.1 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.1 typing-extensions-4.12.2 xxhash-3.5.0 yarl-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sacremoses.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tqdm boto3 requests regex sentencepiece sacremoses datasets numpy pandas torch transformers matplotlib datasets huggingface-hub tokenizers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ssungz\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlib_metadata\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: zipp, importlib_metadata\n",
      "Successfully installed importlib_metadata-8.5.0 zipp-3.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip install importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ssungz/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "unique_labels = set()\n",
    "for split in ['train']:\n",
    "    for item in ds[split]:\n",
    "        unique_labels.add(item['label'])\n",
    "\n",
    "num_classes = len(unique_labels)\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ssungz/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ssungz/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (encoder): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "    self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)['last_hidden_state']\n",
    "    x = self.classifier(x[:, 0])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = TextClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssungz\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2855: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 1574.7437747120857\n",
      "Epoch   1 | Train Loss: 981.5399020016193\n",
      "Epoch   2 | Train Loss: 853.1061174720526\n",
      "Epoch   3 | Train Loss: 796.8456561118364\n",
      "Epoch   4 | Train Loss: 766.7370680868626\n",
      "Epoch   5 | Train Loss: 748.3519441708922\n",
      "Epoch   6 | Train Loss: 732.7270542085171\n",
      "Epoch   7 | Train Loss: 723.1049875468016\n",
      "Epoch   8 | Train Loss: 715.3145329803228\n",
      "Epoch   9 | Train Loss: 707.7595239654183\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lr = 0.0001\n",
    "model = model.to('cuda')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========> Train acc: 0.878 | Test acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "    # preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  train_acc = accuracy(model, train_loader)\n",
    "  test_acc = accuracy(model, test_loader)\n",
    "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
